<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="PhotoPoser">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <!-- <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG"> -->
  <!-- <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PhotoPoster</title>
  <!-- 
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PhotoPoster: A high-fidelity two-stage pose-driven image generation framework</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span> -->
                <a target="_blank">Yanqin Chen</a>,</span>
              <span class="author-block">
                <a target="_blank">Changhao Qiao</a>,</span>
              <span class="author-block">
                <a target="_blank">Yufei Cai</a>,</span>
              <span class="author-block">
                <a target="_blank">Sijie Xu</a>,</span>
              <span class="author-block">
                <a target="_blank">Yang Chen</a>,</span>
              <span class="author-block">
                <a target="_blank">Wei Zhu</a>,</span>
              <span class="author-block">
                <a target="_blank">Dejia Song</a></span>
                  </span>

            </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">dynamicxlab@gmail.com</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                  </div>
                  <span class="link-block">
                    <a href="https://github.com/dynamic-X-LAB/PhotoPoster" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                  <!-- 
                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
            -->
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small"> 
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/all_res1.svg" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <img src="static/images/all_res2.svg" alt="MY ALT TEXT"/>
      </div>
  </div>
</div>
</div>
</section> 
<!-- End image carousel -->







<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <style>
            .custom-line-height p {
              line-height: 1.8; /* 修改此值以调整行间距 */
            }
          </style>
          <div class="custom-line-height">
          <p>
            The remarkable success of diffusion models has sparked increasing research interest in pose-driven visual generation. Despite the impressive results achieved by current methods, they still struggle to preserve the fine details of specific body parts, such as the face and hands, which can easily lead to various distortions and breakdowns. To address this limitation, we propose PhotoPoster, a high-fidelity and flexible two-stage pose-driven image generation framework. Our framework consists of a pre-generation stage and a refinement stage. In the pre-generation stage, we generate a rough pose-driven result. In the refinement stage, we employ a human face restoration network and a hand restoration network to correct distorted faces and hands in most images. PhotoPoster achieves good performance in image-level pose-driven generation tasks. 
          </p>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>
        <div class="content has-text-justified">
        <div style="height: 20px;"></div>


        <div class="content has-text-justified">
          <style>
            .custom-line-height p {
              line-height: 1.8; /* 修改此值以调整行间距 */
            }
          </style>
          <div class="custom-line-height">

          <p>
            Given a human image and a reference pose, PhotoPoster generates high-fidelity, fine-grained pose-driven images through a two-stage process. In the pose-transfer stage, we employ a reference network and a pose guider to inject the human identity texture into the target pose, following the approach of AnimateAnyone. In the subsequent refinement stage, a customized face restoration network is applied to the pre-generated image, re-aligning facial identity and expression details by incorporating a 3D normal map of the input face. Additionally, a ControlNet inpainting model is applied to restore hand distortions and align the hand shape and skin color with the source input.
          </p>
          <img src="static/images/wholenet.png" alt="Teaser Image" height="100%"> 
        </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Face Restoration Net</h2>
        <div class="content has-text-justified">
        <div style="height: 20px;"></div>
        <div class="content has-text-justified">
          <style>
            .custom-line-height p {
              line-height: 1.8; /* 修改此值以调整行间距 */
            }
          </style>
          <div class="custom-line-height">
          <p>
            Due to the relatively small proportion of the facial area in the original image, the generated facial features are prone to fragmentation and expression distortion. To realign the original facial identity during the pose-transfer phase, a facial refinement stage is implemented following the pose-transfer process. The facial area produced during the pose-transfer phase acts as the input face, with the facial region from the original input images used as a reference for ID and expression restoration. 
          </p>
          <img src="static/images/facenet.svg" alt="Teaser Image" height="100%"> 
          <p>
            The face restoration network employs a dual-branch U-Net structure for this purpose. The upper branch features the identity reference U-Net, designed to capture essential identity characteristics from the reference face and introduce them into the lower branch's generation process through an attention mechanism akin to "anymate_anyone". The lower branch of the system encompasses the face restoration U-Net, tasked with reconstructing the facial image from the latent noise input. To ensure accurate face restoration, latent conditions include the background image (excluding the facial region to be restored) and a 3D normal map generated from the original ID reference image and expression image combined. The former provides contextual background information, while the latter conveys intricate facial expression details crucial for the restoration process. These guiding features are extracted by lightweight convolutional feature extractors and integrated into the latent noise prior to iterative denoising. Furthermore, the network incorporates the Face Former module to extract the identity embedding of the reference face, enhancing the preservation of facial identity throughout the restoration process.
          </p>
          </div>
          </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/face_res1.svg" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/face_res2.svg" alt="MY ALT TEXT"/>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Hand Restoration Net</h2>
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <style>
              .custom-line-height p {
                line-height: 1.8; /* 修改此值以调整行间距 */
              }
            </style>
            <div class="custom-line-height">
          <p>
            Hand generation from a single 2D hand pose is a frequently underappreciated yet crucial aspect of pose-driven portrait generation. The task is rendered challenging by the complexities of articulated motion, self-occlusion, and object interaction. Existing methods often struggle to achieve accurate and fine-grained pose alignment, resulting in unnatural partial distortions and failures. To address this issue, we propose a hand inpainting model that incorporates pose condition control. Specifically, we approach hand restoration as an inpainting task. However, traditional inpainting models based on stable diffusion models typically rely on text-based generation conditions, which lack the spatial specificity required to control hand generation. In contrast, our PhotoPoster model utilizes ControlNet to inject pose control conditions into the hand inpainting process. Furthermore, during inpainting, masking the entire human skin area can cause the model to lose prior information about skin color. To mitigate this, we incorporate facial area information to ensure consistent hand skin color.


          </p>
        <img src="static/images/handnet.svg" alt="teaser image" height="100%"> 
          <p>
            We concatenate the noise, mask, and masked images along the channel dimension and feed them into a U-net denoising network with 9 input channels. This part of the input provides background prior information and specifies the location of the inpaint region. To further incorporate hand pose and skin color features into the generation process, we employ two lightweight convolutional layers to extract texture features from the pose and facial images, respectively. These features are then combined and input into a ControlNet network. Notably, the ControlNet network is initialized from a pre-trained 9-channel U-net network. During training, only the parameters of the ControlNet network are updated, while the others remain fixed. The following results demonstrate the effectiveness of our hand inpainting approach.
          </p>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/hand_res11.svg" alt="MY ALT TEXT"/>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/hand_res22.svg" alt="MY ALT TEXT"/>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Limitations and Future Works</h2>
        <div class="content has-text-justified">
          <div class="content has-text-justified">
            <style>
              .custom-line-height p {
                line-height: 1.8; /* 修改此值以调整行间距 */
              }
            </style>
            <div class="custom-line-height">
          <p>
            It is worth noting that, although the refinement stage model can be applied to other pose-driven generation models, it also introduces additional parameters and computational complexity.
            A remaining challenge is to generate high-quality human hands and faces using a single end-to-end model.
            We are currently exploring methods for directly generating high-quality human body regions in an end-to-end manner. Through PhotoPoster, we hope to advance the field of human pose-driven generation.
          </p>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!--BibTex citation -->
<!--
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
-->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
